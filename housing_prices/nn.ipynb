{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'data/train.csv'\n",
    "test_file_path = 'data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (1459, 80) (2919, 79)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "\n",
    "print(train_data.shape, test_data.shape, all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold'],\n",
      "      dtype='object') 36\n"
     ]
    }
   ],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "\n",
    "print(numeric_features, len(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数值特征\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 330)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.get_dummies(all_features, dummy_na=True).astype('float32')\n",
    "print(all_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0673, -0.1844, -0.2178,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.8735,  0.4581, -0.0720,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.0673, -0.0559,  0.1372,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.3025, -0.3986, -0.0784,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0673,  0.6294,  0.5188,  ...,  1.0000,  0.0000,  0.0000]]) tensor([[12.2477],\n",
      "        [12.1090],\n",
      "        [12.3172],\n",
      "        [11.8494],\n",
      "        [12.4292]])\n",
      "torch.Size([1460, 330]) torch.Size([1460, 1]) torch.Size([1459, 330])\n"
     ]
    }
   ],
   "source": [
    "n_train = train_data.shape[0]\n",
    "X_train = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
    "X_test = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data.SalePrice.values, dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.log(y_train)\n",
    "\n",
    "print(X_train[:5], y_train[:5])\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 torch 的 Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([64, 330])\n",
      "Shape of y: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(X_train, y_train)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "  print(f\"Shape of X: {X.shape}\")\n",
    "  print(f\"Shape of y: {y.shape}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(len(all_features.columns), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        price = self.linear_relu_stack(x)\n",
    "        return price\n",
    "\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(len(all_features.columns), 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        price = self.linear(x)\n",
    "        return price\n",
    "    \n",
    "# model = NeuralNetwork().to(device)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     test_pred = torch.exp(model(X_test))\n",
    "#     submission = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_pred.cpu().numpy().flatten()})\n",
    "#     submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_validation(k, X_train, y_train, model, loss_fn, optimizer, epochs):\n",
    "    n = len(X_train)\n",
    "    fold_size = n // k\n",
    "    average_loss = 0\n",
    "    for i in range(k):\n",
    "        print(f\"Fold {i + 1}\")\n",
    "        X_val = X_train[i * fold_size : (i + 1) * fold_size]\n",
    "        y_val = y_train[i * fold_size : (i + 1) * fold_size]\n",
    "        X_train_fold = torch.cat((X_train[: i * fold_size], X_train[(i + 1) * fold_size :]))\n",
    "        y_train_fold = torch.cat((y_train[: i * fold_size], y_train[(i + 1) * fold_size :]))\n",
    "        \n",
    "        train_dataset = TrainDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = TrainDataset(X_val, y_val)\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t + 1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "        print(\"Done!\")\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            loss = loss_fn(val_pred, y_val)\n",
    "            average_loss += loss.item()\n",
    "            print(f\"Validation loss of fold {i + 1}: {loss.item()}\")\n",
    "    average_loss /= k\n",
    "    print(f\"Average validation loss: {average_loss}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_save(X_train, y_train, model, batch_size, loss_fn, optimizer, epochs, submission_file = 'submission.csv'):\n",
    "    train_dataset = TrainDataset(X_train, y_train)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(\"Done!\")\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_pred = torch.exp(model(X_test))\n",
    "        submission = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_pred.cpu().numpy().flatten()})\n",
    "        submission.to_csv(submission_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 144.310913  [   64/  730]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 49.860306  [   64/  730]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 14.263617  [   64/  730]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 4.052580  [   64/  730]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.641578  [   64/  730]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.194674  [   64/  730]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.055496  [   64/  730]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.040431  [   64/  730]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.083757  [   64/  730]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.023521  [   64/  730]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.021919  [   64/  730]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.049409  [   64/  730]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.014869  [   64/  730]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.021600  [   64/  730]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.020070  [   64/  730]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.013977  [   64/  730]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.008060  [   64/  730]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.015634  [   64/  730]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.007683  [   64/  730]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.013279  [   64/  730]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.014985  [   64/  730]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.007031  [   64/  730]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.007751  [   64/  730]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.011632  [   64/  730]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.012498  [   64/  730]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.013138  [   64/  730]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.004760  [   64/  730]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.007535  [   64/  730]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.009304  [   64/  730]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.007100  [   64/  730]\n",
      "Done!\n",
      "Validation loss of fold 1: 0.04995791241526604\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.055549  [   64/  730]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.015789  [   64/  730]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.022031  [   64/  730]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.012359  [   64/  730]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.022994  [   64/  730]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.010345  [   64/  730]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.007062  [   64/  730]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.009619  [   64/  730]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.007519  [   64/  730]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.007690  [   64/  730]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.006438  [   64/  730]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.004811  [   64/  730]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.010171  [   64/  730]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.007949  [   64/  730]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.006879  [   64/  730]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.006549  [   64/  730]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.003382  [   64/  730]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.004842  [   64/  730]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.009344  [   64/  730]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.006200  [   64/  730]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.004881  [   64/  730]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.004322  [   64/  730]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.004059  [   64/  730]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.009173  [   64/  730]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.004734  [   64/  730]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.003115  [   64/  730]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.004083  [   64/  730]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.003808  [   64/  730]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.004257  [   64/  730]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.001921  [   64/  730]\n",
      "Done!\n",
      "Validation loss of fold 2: 0.018849145621061325\n",
      "Average validation loss: 0.03440352901816368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=330, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myMLP = MLP().to(device)\n",
    "k_fold_validation(2, X_train, y_train, myMLP, nn.MSELoss(), torch.optim.Adam(myMLP.parameters(), lr=1e-3), 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 145.260834  [   64/  730]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 132.336273  [   64/  730]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 123.003494  [   64/  730]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 110.017754  [   64/  730]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 101.775375  [   64/  730]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 92.291176  [   64/  730]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 82.063210  [   64/  730]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 75.232544  [   64/  730]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 65.829102  [   64/  730]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 59.407452  [   64/  730]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 53.584278  [   64/  730]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 47.778667  [   64/  730]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 42.749374  [   64/  730]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 38.334534  [   64/  730]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 33.299732  [   64/  730]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 29.285683  [   64/  730]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 25.889181  [   64/  730]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 22.828484  [   64/  730]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 20.226725  [   64/  730]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 17.971287  [   64/  730]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 15.552380  [   64/  730]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 13.230165  [   64/  730]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 11.323337  [   64/  730]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 9.881432  [   64/  730]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 8.190925  [   64/  730]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 7.040099  [   64/  730]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 6.312891  [   64/  730]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 5.305376  [   64/  730]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 4.441938  [   64/  730]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.806809  [   64/  730]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.285059  [   64/  730]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 2.571626  [   64/  730]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 2.442980  [   64/  730]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 1.943750  [   64/  730]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 1.524977  [   64/  730]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 1.380582  [   64/  730]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 1.085809  [   64/  730]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.937878  [   64/  730]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.925947  [   64/  730]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.575868  [   64/  730]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.447872  [   64/  730]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.504422  [   64/  730]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.508176  [   64/  730]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.321396  [   64/  730]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.332972  [   64/  730]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.265061  [   64/  730]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.265923  [   64/  730]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.162251  [   64/  730]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.148340  [   64/  730]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.109938  [   64/  730]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.138192  [   64/  730]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.118888  [   64/  730]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.094401  [   64/  730]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.135157  [   64/  730]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.075093  [   64/  730]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.061218  [   64/  730]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.085892  [   64/  730]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.049238  [   64/  730]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.062685  [   64/  730]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.036078  [   64/  730]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.064094  [   64/  730]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.108684  [   64/  730]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.081560  [   64/  730]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.044757  [   64/  730]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.031606  [   64/  730]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.049149  [   64/  730]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.069804  [   64/  730]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.043469  [   64/  730]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.075804  [   64/  730]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.038245  [   64/  730]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.071016  [   64/  730]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.069653  [   64/  730]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.069779  [   64/  730]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.085693  [   64/  730]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.051717  [   64/  730]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.045745  [   64/  730]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.047824  [   64/  730]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.039213  [   64/  730]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.048737  [   64/  730]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.040645  [   64/  730]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.040987  [   64/  730]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.043786  [   64/  730]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.029693  [   64/  730]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.051275  [   64/  730]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.046854  [   64/  730]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.031622  [   64/  730]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.086937  [   64/  730]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.039336  [   64/  730]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.021723  [   64/  730]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.038153  [   64/  730]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.034941  [   64/  730]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.025751  [   64/  730]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.053126  [   64/  730]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.041166  [   64/  730]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.034089  [   64/  730]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.040135  [   64/  730]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.036262  [   64/  730]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.035458  [   64/  730]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.028491  [   64/  730]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.037446  [   64/  730]\n",
      "Done!\n",
      "Validation loss of fold 1: 0.050184767693281174\n",
      "Fold 2\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.084126  [   64/  730]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.045789  [   64/  730]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.044504  [   64/  730]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.030172  [   64/  730]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.045831  [   64/  730]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.039078  [   64/  730]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.048280  [   64/  730]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.031505  [   64/  730]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.058034  [   64/  730]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.040780  [   64/  730]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.017090  [   64/  730]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.021568  [   64/  730]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.024896  [   64/  730]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.041892  [   64/  730]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.041869  [   64/  730]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.032818  [   64/  730]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.031802  [   64/  730]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.033946  [   64/  730]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.043005  [   64/  730]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.047087  [   64/  730]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.025951  [   64/  730]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.015209  [   64/  730]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.017061  [   64/  730]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.019636  [   64/  730]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.027613  [   64/  730]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.029573  [   64/  730]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.022421  [   64/  730]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.018467  [   64/  730]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.024815  [   64/  730]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.027382  [   64/  730]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.014349  [   64/  730]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.025180  [   64/  730]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.028846  [   64/  730]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.022963  [   64/  730]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.036742  [   64/  730]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.025168  [   64/  730]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.023374  [   64/  730]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.019590  [   64/  730]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.015627  [   64/  730]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.017511  [   64/  730]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.020789  [   64/  730]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.021149  [   64/  730]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.021701  [   64/  730]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.019208  [   64/  730]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.018826  [   64/  730]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.023008  [   64/  730]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.018816  [   64/  730]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.022808  [   64/  730]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.013554  [   64/  730]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.018663  [   64/  730]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.015305  [   64/  730]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.017454  [   64/  730]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.029964  [   64/  730]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.023254  [   64/  730]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.017587  [   64/  730]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.028863  [   64/  730]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.014714  [   64/  730]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.011593  [   64/  730]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.016150  [   64/  730]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.017479  [   64/  730]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.012041  [   64/  730]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.015042  [   64/  730]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.034622  [   64/  730]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.016648  [   64/  730]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.016412  [   64/  730]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.021318  [   64/  730]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.006985  [   64/  730]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.014083  [   64/  730]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.019980  [   64/  730]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.020485  [   64/  730]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.021652  [   64/  730]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.019935  [   64/  730]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.033173  [   64/  730]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.025499  [   64/  730]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.017253  [   64/  730]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.014616  [   64/  730]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.013088  [   64/  730]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.013037  [   64/  730]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.015615  [   64/  730]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.017173  [   64/  730]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.013138  [   64/  730]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.013727  [   64/  730]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.015788  [   64/  730]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.015571  [   64/  730]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.015392  [   64/  730]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.015748  [   64/  730]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.013352  [   64/  730]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.039059  [   64/  730]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.010303  [   64/  730]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.014372  [   64/  730]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.013447  [   64/  730]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.016580  [   64/  730]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.009725  [   64/  730]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.009609  [   64/  730]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.017192  [   64/  730]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.012292  [   64/  730]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.039659  [   64/  730]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.008169  [   64/  730]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.008418  [   64/  730]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.010509  [   64/  730]\n",
      "Done!\n",
      "Validation loss of fold 2: 0.022511333227157593\n",
      "Average validation loss: 0.03634805046021938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(\n",
       "  (linear): Linear(in_features=330, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myLinear = LinearRegression().to(device)\n",
    "k_fold_validation(2, X_train, y_train, myLinear, nn.MSELoss(), torch.optim.Adam(myLinear.parameters(), lr=1e-3), 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
