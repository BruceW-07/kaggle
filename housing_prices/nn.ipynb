{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = 'data/train.csv'\n",
    "test_file_path = 'data/test.csv'\n",
    "\n",
    "train_data = pd.read_csv(train_file_path)\n",
    "test_data = pd.read_csv(test_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1460, 81) (1459, 80) (2919, 79)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))\n",
    "\n",
    "print(train_data.shape, test_data.shape, all_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond',\n",
      "       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n",
      "       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n",
      "       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n",
      "       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n",
      "       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n",
      "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n",
      "       'MoSold', 'YrSold'],\n",
      "      dtype='object') 36\n"
     ]
    }
   ],
   "source": [
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "\n",
    "print(numeric_features, len(numeric_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化数值特征\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2919, 330)\n"
     ]
    }
   ],
   "source": [
    "all_features = pd.get_dummies(all_features, dummy_na=True).astype('float32')\n",
    "print(all_features.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0673, -0.1844, -0.2178,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [-0.8735,  0.4581, -0.0720,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.0673, -0.0559,  0.1372,  ...,  1.0000,  0.0000,  0.0000],\n",
      "        [ 0.3025, -0.3986, -0.0784,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0673,  0.6294,  0.5188,  ...,  1.0000,  0.0000,  0.0000]]) tensor([[12.2477],\n",
      "        [12.1090],\n",
      "        [12.3172],\n",
      "        [11.8494],\n",
      "        [12.4292]])\n",
      "torch.Size([1460, 330]) torch.Size([1460, 1]) torch.Size([1459, 330])\n"
     ]
    }
   ],
   "source": [
    "n_train = train_data.shape[0]\n",
    "X_train = torch.tensor(all_features[:n_train].values, dtype=torch.float32)\n",
    "X_test = torch.tensor(all_features[n_train:].values, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_data.SalePrice.values, dtype=torch.float32).view(-1, 1)\n",
    "y_train = torch.log(y_train)\n",
    "\n",
    "print(X_train[:5], y_train[:5])\n",
    "print(X_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立 torch 的 Dataset 和 DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        assert len(X) == len(y)\n",
    "        self.X, self.y = X, y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: torch.Size([64, 330])\n",
      "Shape of y: torch.Size([64, 1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TrainDataset(X_train, y_train)\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for X, y in train_dataloader:\n",
    "  print(f\"Shape of X: {X.shape}\")\n",
    "  print(f\"Shape of y: {y.shape}\")\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=330, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # self.linear_relu_stack = nn.Sequential(\n",
    "        #     nn.Linear(len(all_features.columns), 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(256, 256),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Linear(256, 1)\n",
    "        # )\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(X_train.shape[1], 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        price = self.linear_relu_stack(x)\n",
    "        return price\n",
    "    \n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.977460  [   64/ 1460]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.707125  [   64/ 1460]\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.028079  [   64/ 1460]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.411597  [   64/ 1460]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.077563  [   64/ 1460]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.618348  [   64/ 1460]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.531889  [   64/ 1460]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.313209  [   64/ 1460]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.296743  [   64/ 1460]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.160741  [   64/ 1460]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.124895  [   64/ 1460]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.138639  [   64/ 1460]\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.086676  [   64/ 1460]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.083841  [   64/ 1460]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.088262  [   64/ 1460]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.093076  [   64/ 1460]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.076355  [   64/ 1460]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.069723  [   64/ 1460]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.050787  [   64/ 1460]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.036382  [   64/ 1460]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.051207  [   64/ 1460]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.040625  [   64/ 1460]\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.047278  [   64/ 1460]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.055156  [   64/ 1460]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.027922  [   64/ 1460]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.040356  [   64/ 1460]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.033758  [   64/ 1460]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.053509  [   64/ 1460]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.027917  [   64/ 1460]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.038544  [   64/ 1460]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.038069  [   64/ 1460]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.045868  [   64/ 1460]\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.031289  [   64/ 1460]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.036838  [   64/ 1460]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.032950  [   64/ 1460]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.064070  [   64/ 1460]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.016148  [   64/ 1460]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.025649  [   64/ 1460]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.027822  [   64/ 1460]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.029249  [   64/ 1460]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.028588  [   64/ 1460]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.020182  [   64/ 1460]\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.034323  [   64/ 1460]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.031017  [   64/ 1460]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.025805  [   64/ 1460]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.033372  [   64/ 1460]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.032128  [   64/ 1460]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.017559  [   64/ 1460]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.034306  [   64/ 1460]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.020473  [   64/ 1460]\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.019805  [   64/ 1460]\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.026446  [   64/ 1460]\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.030652  [   64/ 1460]\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.023487  [   64/ 1460]\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.020174  [   64/ 1460]\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.024672  [   64/ 1460]\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.032305  [   64/ 1460]\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.029700  [   64/ 1460]\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.034127  [   64/ 1460]\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.017007  [   64/ 1460]\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.030108  [   64/ 1460]\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.020258  [   64/ 1460]\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.016405  [   64/ 1460]\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.049735  [   64/ 1460]\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.027588  [   64/ 1460]\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.017953  [   64/ 1460]\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.021289  [   64/ 1460]\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.020182  [   64/ 1460]\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.023612  [   64/ 1460]\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.020992  [   64/ 1460]\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.015544  [   64/ 1460]\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.031371  [   64/ 1460]\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.015858  [   64/ 1460]\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.017755  [   64/ 1460]\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.026714  [   64/ 1460]\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.020566  [   64/ 1460]\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.011400  [   64/ 1460]\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.017824  [   64/ 1460]\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.014113  [   64/ 1460]\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.013899  [   64/ 1460]\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.010503  [   64/ 1460]\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.024175  [   64/ 1460]\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.022976  [   64/ 1460]\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.020197  [   64/ 1460]\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.016724  [   64/ 1460]\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.010938  [   64/ 1460]\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.015603  [   64/ 1460]\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.014765  [   64/ 1460]\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.019378  [   64/ 1460]\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.019380  [   64/ 1460]\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.017914  [   64/ 1460]\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.011771  [   64/ 1460]\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.012906  [   64/ 1460]\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.017049  [   64/ 1460]\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.012829  [   64/ 1460]\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.013194  [   64/ 1460]\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.022162  [   64/ 1460]\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.023241  [   64/ 1460]\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.019667  [   64/ 1460]\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.014460  [   64/ 1460]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 12.276256561279297, Actual: 12.24769401550293\n",
      "Predicted: 12.017802238464355, Actual: 12.109010696411133\n",
      "Predicted: 12.324498176574707, Actual: 12.317166328430176\n",
      "Predicted: 12.128534317016602, Actual: 11.849397659301758\n",
      "Predicted: 12.594012260437012, Actual: 12.429216384887695\n",
      "Predicted: 11.935307502746582, Actual: 11.870599746704102\n",
      "Predicted: 12.52228832244873, Actual: 12.634603500366211\n",
      "Predicted: 12.100232124328613, Actual: 12.206072807312012\n",
      "Predicted: 11.830005645751953, Actual: 11.774519920349121\n",
      "Predicted: 11.4766845703125, Actual: 11.67844009399414\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    train_pred = model(X_train[:10])\n",
    "    for i in range(10):\n",
    "        print(f\"Predicted: {train_pred[i].item()}, Actual: {y_train[i].item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_pred = torch.exp(model(X_test))\n",
    "    submission = pd.DataFrame({'Id': test_data['Id'], 'SalePrice': test_pred.cpu().numpy().flatten()})\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     OverallCond  FullBath  HalfBath  KitchenAbvGr  YearRemodAdd  3SsnPorch  \\\n",
      "660            7         1         0             1          1950          0   \n",
      "\n",
      "     TotRmsAbvGrd  WoodDeckSF  BedroomAbvGr  MiscVal  ...  OverallQual  \\\n",
      "660             4           0             2        0  ...            4   \n",
      "\n",
      "     LowQualFinSF  2ndFlrSF  EnclosedPorch  YearBuilt  Fireplaces  YrSold  \\\n",
      "660             0         0              0       1946           0    2008   \n",
      "\n",
      "     ScreenPorch  OpenPorchSF  MoSold  \n",
      "660            0            0       4  \n",
      "\n",
      "[1 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[test_data['Id'] == 2121][house_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
